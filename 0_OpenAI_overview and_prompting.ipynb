{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81b97a3-1e02-418e-bf87-2117ef4339b9",
   "metadata": {},
   "source": [
    "# OpenAI models and Prompting:  \n",
    "In this notebook we will cover these aspects:  \n",
    "* **Tokenization, basics:** What is tokenization and why it is important for Large Language Models (LLM) for understaing a text\n",
    "* **OpenAI LLM models overview:** Types of models and their APIs in OpenAI\n",
    "* **Prompt Engineering:** Some vital prompt eng. techniques and examples for some DS/ML tasks\n",
    "* **Tokenization, advanced:** Tokenization methods and examples with `tiktoken`\n",
    "* **Calculate costs for models and prompts:** Functions for calculating number of tokens and costs for OpenAI models\n",
    "\n",
    "\n",
    "Let's begin with tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e3c42-8a29-4c42-8501-5dedbf8805df",
   "metadata": {},
   "source": [
    "# Tokenization, basics:  \n",
    "When a text is passed to an OpenAI model, it is split into parts, called `tokens`. It's done with `tokenizer`.   \n",
    "\n",
    "For example:      \n",
    "Given a text string (e.g., `\"tiktoken is great!\"`) a tokenizer can split the text string into a list of tokens (e.g., `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
    "\n",
    "Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token).  \n",
    "For example, the longest word in English, `pneumonoultramicroscopicsilicovolcanoconiosis` tokenized with [OpenAI Tokenizer](https://beta.openai.com/tokenizer) would have:  \n",
    "* 15 Tokens  \n",
    "* 45 chars  \n",
    "\n",
    "![title](img/tokenization.png)  \n",
    "\n",
    "You can see different colors represent different tokens that the original word was split into. If you click on `TOKEN IDS` button, the tokens will be represented as list of numbers. These numbers will be passed to a model instead of string tokens, because the model itself cannot recognize strings, but only numbers.  \n",
    "![title](img/tokenization_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fd718-9120-479f-9471-41076fb42ea3",
   "metadata": {},
   "source": [
    "More about working with token IDs will be covered in `Tokenization, advanced` section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7936c-a51b-4a21-b35a-db24cf8e6d1f",
   "metadata": {},
   "source": [
    "# OpenAI LLM models overview:  \n",
    "There are 2 major versions of LLM models: [GPT-3](https://platform.openai.com/docs/models/gpt-3) and [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5). Basically, `GPT-3.5` models outperform `GPT-3`, but cost higher, so it's reasonable to select a model according to the task that you want to solve.  \n",
    "\n",
    "Here we will not cover `GPT-4` since it's still on limited beta (June, 2023). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde067b9-1f3c-4fc6-ba47-4bdea0a5404a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595ea74f-445a-41a1-97aa-033b0e68e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = \"YOUR_OPENAI_TOKEN\"\n",
    "\n",
    "## Additional parameters if you use Azure backend (uncomment if you do):\n",
    "# openai.api_base = \"HTTPS to your Azure endpoint\"\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Be aware, this notebook is not written for Azure backend, so, please, \n",
    "# when using Completion or ChatCompletion model APIs, replace `model` parameter\n",
    "# with `endpoint` or `deployment_id` and specify your deployment name.\n",
    "# Otherwise, you will get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdad748-248a-434e-a62d-5a7b7d8f0346",
   "metadata": {},
   "source": [
    "## GPT-3 models:\n",
    "**GPT-3** models are: `ada`, `babbage`, `curie`, their `text-` variants and `davinci` . Only original models like `ada` (without prefix `text-`) available for fine-tuning, and their `text-` variants could be used as is.  \n",
    "There is a description from OpenAI documentation (June, 2023):  \n",
    "\n",
    "![Screenshot 2023-06-29 at 20.44.45.png](./img/openaicompletion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05532cfd-eed4-4865-9a2b-a3bcda618999",
   "metadata": {},
   "source": [
    "\n",
    "Different models are capable of solving different tasks:\n",
    "* **ada** -- binary, multiclass classification of text; parsing and information extraction; very fast but not so powerful;  \n",
    "* **babbage** -- all of the above + excels in semantic search like 'how well texts match a query text';\n",
    "* **curie** -- quite a balanced model in terms of latency and performance; summarization, QA, complex classification + babbage's tasks;  \n",
    "* **davinci** -- has the best quality and text comprehension across all other models.  \n",
    "\n",
    "Having this information and a task that you should solve, you can choose the most suitable model for it.  \n",
    "For example, it's **ada** could be not the best choice if you want to solve QA task and you'd rather try **curie**. But for the binary classification of text you can fine-tune **ada** and get quite good results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fd50b-0a4c-4d63-aa8e-5e113f8db9aa",
   "metadata": {},
   "source": [
    "## GPT-3.5:  \n",
    "These models include `text-davinci-*`, `code-davinci` variants, `gpt-3.5-*` and `gpt-4-*`.  \n",
    "There is a description from OpenAI documentation (June, 2023):  \n",
    "  \n",
    "  \n",
    "![Screenshot 2023-06-29 at 21.03.27.png](./img/gpt35models.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b779d-ef41-4b2c-96d0-2473eb3b97ea",
   "metadata": {},
   "source": [
    "\n",
    "Basically, `gpt-3.5-turbo` is cheaper than `davinci` and was fine-tuned on the chat data, so that's the best choice if you're building chatbot or want to solve NLP tasks with decent quality. OpenAI often releases new version of `gpt-3.5`, like `gpt-3.5-turbo-0613` adding new features. After some time a newest model will replace `gpt-3.5-turbo`, meaning when you will use `gpt-3.5-turbo` you will use the most recent model automatically. As you can see, starting from 27th of June, `gpt-3.5-turbo` will be replaced by `gpt-3.5-turbo-0613`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d19e0b-73ea-45b9-aa45-60e597704dca",
   "metadata": {},
   "source": [
    "## Completion VS ChatCompletion API:  \n",
    "There is also 2 variants of API or how to use these models, for example in Python: `Completion` and `ChatCompletion`.  \n",
    "Basically, `ChatCompletion` is `gpt-3.5-*` and higher, like `gpt-4`. Everything else, even davinci, are `Completion` models.\n",
    "\n",
    "## [Completion API](https://platform.openai.com/docs/guides/gpt/completions-api):  \n",
    "For the most common case you need to specify:\n",
    "* model name\n",
    "* prompt that will have the task for the model and should be a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5188daae-4be3-475e-81cb-818c1e2dd153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    # deployment_id=\"deployment_with_davinci\",\n",
    "    prompt=\"Write a tagline for an ice cream shop.\",\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b3c17ec-304b-4ddc-a6f3-4a09e02fe00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7Ws0kzlg2KOTmmpDOJX2ARKqjUJxK\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1688068470,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n\\\"Cool down with our delicious treats!\\\"\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 10,\n",
      "    \"completion_tokens\": 10,\n",
      "    \"total_tokens\": 20\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example of an output JSON:\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8658708a-2bc1-4aab-a831-92b9a4d33abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Taste the chill of happiness!\"\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009b007-9b37-4deb-9605-91b4be5fee58",
   "metadata": {},
   "source": [
    "Let's cover most important parameters for API:  \n",
    "- `temperature` -- aka randomness or 'creativity' of an output, `[0, 1]`. 0 -- no random, the output will be the same all the executions. 1 -- the output will be very different each execution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b50c212a-0b03-4a0d-820f-ec9ae34258d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Taste the Cream of the Crop!\"\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Write a tagline for an ice cream shop.\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=256,\n",
    "    n=1,\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a3580-95ff-4c61-853e-c35a5295f76e",
   "metadata": {},
   "source": [
    "* `max_tokens` -- limits number of tokens in the output. It works like too low values will shrink your output, but not making the model to generate very short output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd5b020d-4b5b-4aef-951e-32147d3f49f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It's\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Write a tagline for an ice cream shop.\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=5,\n",
    "    n=1,\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a6675-38e5-434b-8882-9b972f35a871",
   "metadata": {},
   "source": [
    "* `n` -- how many output variants to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4850b1ad-4e19-4066-8300-7ef34f320c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<OpenAIObject at 0x7f9288e8fe70> JSON: {\n",
      "  \"text\": \"\\n\\n\\\"The sweetest treat to cool you down!\\\"\",\n",
      "  \"index\": 0,\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"stop\"\n",
      "}, <OpenAIObject at 0x7f9288e8f470> JSON: {\n",
      "  \"text\": \"\\n\\n\\\"Cool off with a scoop of deliciousness!\\\"\",\n",
      "  \"index\": 1,\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"stop\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Write a tagline for an ice cream shop.\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=256,\n",
    "    n=2,\n",
    ")\n",
    "print(response[\"choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b39b8-1b8f-4e01-acc4-f89d2bfcbdd4",
   "metadata": {},
   "source": [
    "Be aware, it's meaningless to use `n > 1` and `temperature == 0` since you will get `n` equal results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f750557-33db-475f-92b0-8a2f02b5cea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<OpenAIObject at 0x7f9288e8e2f0> JSON: {\n",
      "  \"text\": \"\\n\\n\\\"Cool down with our delicious treats!\\\"\",\n",
      "  \"index\": 0,\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"stop\"\n",
      "}, <OpenAIObject at 0x7f9288e8e430> JSON: {\n",
      "  \"text\": \"\\n\\n\\\"Cool down with our delicious treats!\\\"\",\n",
      "  \"index\": 1,\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"stop\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Write a tagline for an ice cream shop.\",\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    n=2,\n",
    ")\n",
    "print(response[\"choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd35cf0-4221-4f8f-bc60-d1b4adaa1bfa",
   "metadata": {},
   "source": [
    "Other parameters you can check on official [OpenAI page](https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens:~:text=given%20chat%20conversation.-,Request%20body,-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932a791-66c6-4170-a882-8a0f62466ebd",
   "metadata": {},
   "source": [
    "## [ChatCompletion API](https://platform.openai.com/docs/guides/gpt/chat-completions-api):  \n",
    "\n",
    "For the most common case you need to specify:\n",
    "* model name \n",
    "* messages in the form of list of dictionaries; they will emulate a chat between a user and an assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c970684-b402-4c3f-a06a-54847feb19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35698270-1d09-4247-b790-2ea51dc506f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7WrzGWs27LSI9IH2Oo6DBUmQCguD7\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1688068378,\n",
      "  \"model\": \"gpt-35-turbo\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 57,\n",
      "    \"completion_tokens\": 19,\n",
      "    \"total_tokens\": 76\n",
      "  },\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas, USA.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example of an output JSON:\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d9403ca-f718-4f3b-bf84-482e2c4fb81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played at Globe Life Field in Arlington, Texas, USA.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104baad5-b52f-4a55-8c38-339ebe8aea65",
   "metadata": {},
   "source": [
    "The **role** here is specifying who exactly sends the message, whether its from Human or from AI.  \n",
    "- **system role** is not required, but helpful to specify how a model should consider itself.  \n",
    "[From OpenAI documentation](https://platform.openai.com/docs/guides/gpt/chat-completions-api#:~:text=The%20system%20message,a%20helpful%20assistant.%22):  \n",
    "\"The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model‚Äôs behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"  \n",
    "- **human role** are possible inputs from the user helping a model to 'fit' on them \n",
    "- **assistant** -- examples of a desired behavior on the user's inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f7066-6ef6-4bf2-baf6-717e17467546",
   "metadata": {},
   "source": [
    "## OpenAI Embedding models overview:  \n",
    "You can use OpenAI models to vectorize texts, for example for QA, semantic search, topic recognition and other tasks that require calculations of similarities of texts.  \n",
    "[OpenAI Documentation link](https://platform.openai.com/docs/guides/embeddings/embedding-models).  \n",
    "\n",
    "![Screenshot 2023-06-29 at 21.39.27.png](./img/embeddings.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd478a4-3866-4b8c-ae6b-02c956fd817a",
   "metadata": {},
   "source": [
    "Basically, everything, except `text-embedding-ada-002`, is first-generation model and they are outperformed by `*-ada-002`.  \n",
    "There is a simple example of obtaining embeddings for a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1b764ed-33da-4e5e-917a-29cdb63690e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "text_to_embed = \"This text will be vectorized and used in some DS tasks...\"\n",
    "embedding = get_embedding(text_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b6fe3a4-b639-46a4-b425-d5a3dc05d1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685fa95-b68d-402d-82ef-e291d9e00bd5",
   "metadata": {},
   "source": [
    "So far we covered different models from OpenAI, there is a short summary:\n",
    "* Completion models -- most of them not very powerful, but fast and capable of very simple tasks (except Davinci model);\n",
    "* ChatCompletion models -- `gpt-35-turbo` could be a very good baseline in most of the cases;\n",
    "* Embedding models -- you may want to use `text-embedding-ada-002` since it's not very pricy and has a good quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f09c1-4c98-4e40-8e5d-939e4727fa1d",
   "metadata": {},
   "source": [
    "# üìù Prompt Engineering:  \n",
    "In this section we will cover:\n",
    "* Existing best techniques for prompt engineerging\n",
    "* Completion prompts for some Data Science tasks  \n",
    "\n",
    "\n",
    "*[References] This section is based on [DeepLearning.ai course](https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction?_gl=1*tjnub9*_ga*MzgyMDU1MDc3LjE2ODMwNTU2MzE.*_ga_PZF1GBS1R1*MTY4Nzk0NjM3My45LjEuMTY4Nzk0Nzk4Ni4zOC4wLjA.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38b8e7fb-57c5-439f-9397-6a8b31c84c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str):\n",
    "    response = openai.Completion.create(\n",
    "          model=\"text-davinci-003\"\n",
    "          prompt=prompt,\n",
    "          temperature=0,\n",
    "          max_tokens=256,\n",
    "          top_p=1,\n",
    "          frequency_penalty=0.0,\n",
    "          presence_penalty=0.0,\n",
    "        )\n",
    "    \n",
    "    return response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a359a-566e-4af0-ba66-219e36a77283",
   "metadata": {},
   "source": [
    "## Best practices:  \n",
    "Now let's observe and try different techniques helping you to construct a comprehensive and effective prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a877b-4474-4d6f-a3a9-b3f8d2d20304",
   "metadata": {},
   "source": [
    "**1. Use delimiters to indicate distinct parts of the input clearly:**\n",
    "\n",
    "Delimiters can be anything like: ```, \"\"\", < >,¬†\\<tag> \\</tag>. It could help model focusing on specific parts of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be353bde-5609-4d1f-907c-46c40835790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is the fifth planet from the Sun, a gas giant with a mass two-and-a-half times that of all the other planets in the Solar System combined, and is one of the brightest objects visible to the naked eye in the night sky, having been known to ancient civilizations since before recorded history.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the largest in the Solar System.\n",
    "It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half \n",
    "times that of all the other planets in the Solar System combined. \n",
    "Jupiter is one of the brightest objects visible to the naked eye in the night sky,\n",
    "and has been known to ancient civilizations since before recorded history. \n",
    "It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be \n",
    "bright enough for its reflected light to cast visible shadows,[20] and is on average \n",
    "the third-brightest natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Summarize the text delimited by triple backticks into a single sentence. ```{text}``` \"\"\"\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b42760-edf8-422a-9f91-c9ade8d2daca",
   "metadata": {},
   "source": [
    "**2. Ask for a structured output:**\n",
    "\n",
    "You can as about JSON, HTML or any other reasonable format. JSON could be useful in many cases, let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "240f6033-c796-430f-a444-ff419ed16f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"book_id\": 1,\n",
      "        \"title\": \"The Adventures of the Lost Princess\",\n",
      "        \"author\": \"J.K. Rowling\",\n",
      "        \"genre\": \"Fantasy\"\n",
      "    },\n",
      "    {\n",
      "        \"book_id\": 2,\n",
      "        \"title\": \"The Mystery of the Missing Heirloom\",\n",
      "        \"author\": \"Agatha Christie\",\n",
      "        \"genre\": \"Mystery\"\n",
      "    },\n",
      "    {\n",
      "        \"book_id\": 3,\n",
      "        \"title\": \"The Rise of the Superheroes\",\n",
      "        \"author\": \"Stan Lee\",\n",
      "        \"genre\": \"Superhero\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\" \n",
    "Generate a list of three made-up book titles along \n",
    "with their authors and genres. Provide them in JSON \n",
    "format with the following keys: book_id, title, author, genre. \n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4c9fa-6d5a-4705-a32d-f319a40646de",
   "metadata": {},
   "source": [
    "**3. Ask the model to check whether conditions are satisfied:**  \n",
    "It's like `IF-ELSE` statement inside a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b04ccb70-b075-4290-b5fb-e63568e6f68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps provided:  Step 1 - Get some water boiling. Step 2 - Grab a cup and put a tea bag in it. Step 3 - Pour the hot water over the tea bag. Step 4 - Let the tea steep for a few minutes. Step 5 - Take out the tea bag. Step 6 - Add sugar or milk to taste. Step 7 - Enjoy your cup of tea.\n",
      "Steps missed:  No steps provided.\n"
     ]
    }
   ],
   "source": [
    "recipe_w_steps = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some\n",
    " water boiling. While that's happening, \n",
    "grab a cup and put a tea bag in it. Once the water is \n",
    "hot enough, just pour it over the tea bag. \n",
    "Let it sit for a bit so the tea can steep. After a \n",
    "few minutes, take out the tea bag. If you \n",
    "like, you can add some sugar or milk to taste. \n",
    "And that's it! You've got yourself a delicious \n",
    "cup of tea to enjoy. \n",
    "\"\"\" \n",
    "\n",
    "recipe_wo_steps = f\"\"\"\n",
    "Making a cup of tea is easy! Just do it!\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "prompt = \"\"\" \n",
    "You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ... Step 2 - ‚Ä¶ ‚Ä¶ Step N - ‚Ä¶\n",
    "\n",
    "If the text does not contain a sequence of instructions, \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{recipe}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"Steps provided: \", generate_response(prompt=prompt.format(recipe=recipe_w_steps)))\n",
    "print(\"Steps missed: \", generate_response(prompt=prompt.format(recipe=recipe_wo_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37bf1a7-8869-45af-aaa2-a2897a17939c",
   "metadata": {},
   "source": [
    "**4. Focus on specific aspects:**  \n",
    "Specifically ask a model to pay more attention on some details according to the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90df4576-bdc8-47a2-ae07-c10ead40b45d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This mid-century inspired office chair is constructed from cast aluminum with a modified nylon PA6/PA66 coating and 10mm shell thickness. It features a 5-wheel plastic coated aluminum base and pneumatic chair adjust for easy raise/lower action. Choose from several shell color and base finishes, with or without armrests, and two choices of seat foam densities. Suitable for home or business settings, this chair is made in Italy and qualified for contract use.\n"
     ]
    }
   ],
   "source": [
    "fact_sheet_chair = \"\"\"\n",
    "OVERVIEW\n",
    "- Part of a beautiful family of mid-century inspired office furniture, \n",
    "including filing cabinets, desks, bookcases, meeting tables, and more.\n",
    "- Several options of shell color and base finishes.\n",
    "- Available with plastic back and front upholstery (SWC-100) \n",
    "or full upholstery (SWC-110) in 10 fabric and 6 leather options.\n",
    "- Base finish options are: stainless steel, matte black, \n",
    "gloss white, or chrome.\n",
    "- Chair is available with or without armrests.\n",
    "- Suitable for home or business settings.\n",
    "- Qualified for contract use.\n",
    "\n",
    "CONSTRUCTION\n",
    "- 5-wheel plastic coated aluminum base.\n",
    "- Pneumatic chair adjust for easy raise/lower action.\n",
    "\n",
    "DIMENSIONS\n",
    "- WIDTH 53 CM | 20.87‚Äù\n",
    "- DEPTH 51 CM | 20.08‚Äù\n",
    "- HEIGHT 80 CM | 31.50‚Äù\n",
    "- SEAT HEIGHT 44 CM | 17.32‚Äù\n",
    "- SEAT DEPTH 41 CM | 16.14‚Äù\n",
    "\n",
    "OPTIONS\n",
    "- Soft or hard-floor caster options.\n",
    "- Two choices of seat foam densities: \n",
    " medium (1.8 lb/ft3) or high (2.8 lb/ft3)\n",
    "- Armless or 8 position PU armrests \n",
    "\n",
    "MATERIALS\n",
    "SHELL BASE GLIDER\n",
    "- Cast Aluminum with modified nylon PA6/PA66 coating.\n",
    "- Shell thickness: 10 mm.\n",
    "SEAT\n",
    "- HD36 foam\n",
    "\n",
    "COUNTRY OF ORIGIN\n",
    "- Italy\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Your task is to help a marketing team create a description for a retail website of a product based on a technical fact sheet.\n",
    "Write a product description based on the information provided in the technical specifications delimited by triple backticks.\n",
    "\n",
    "The description is intended for furniture retailers, so should be technical in nature and focus on the materials the product is constructed from.\n",
    "\n",
    "Use at most 50 words.\n",
    "Technical specifications: ```{fact_sheet_chair}```\n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b93c94-6075-47b1-83dc-fe8b0d8c26cb",
   "metadata": {},
   "source": [
    "**5. Specify the model‚Äôs role:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c95ca766-462b-413a-a8e6-0d1e7f91110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome to OrderBot. What can I get for you today?\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You are OrderBot, an automated service to collect orders for a pizza restaurant. \n",
    "You first greet the customer, then collects the order, and then asks if it's a pickup or delivery. \n",
    "You wait to collect the entire order, then summarize it and check for a final time if the customer wants to add anything else.\n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dec9e3-089a-437f-91d2-50ab80475ef5",
   "metadata": {},
   "source": [
    "## Prompt examples for Data Science / Machine Learning tasks:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f847ed1-fecd-430d-a7bd-4482bf6002ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_review = \"\"\"\n",
    "Got this panda plush toy for my daughter's birthday, \\\n",
    "who loves it and takes it everywhere. It's soft and \\ \n",
    "super cute, and its face has a friendly look. It's \\ \n",
    "a bit small for what I paid though. I think there \\ \n",
    "might be other options that are bigger for the \\ \n",
    "same price. It arrived a day earlier than expected, \\ \n",
    "so I got to play with it myself before I gave it \\ \n",
    "to her.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7cade3-e51f-4033-9156-bfd473c15dd9",
   "metadata": {},
   "source": [
    "**1. Summarization with focus:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fc02f0a-91b1-4239-94db-98a281ab52d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft and cute panda plush toy was a hit with daughter, but was smaller than expected for the price. Arrived a day early.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\" \n",
    "Your task is to generate a short summary of a product review from an ecommerce site\n",
    "to give feedback to the pricing deparmtment, responsible for determining the price of the product.\n",
    "\n",
    "Summarize the review below, delimited by triple backticks, in at most 30 words, and focusing on any aspects \n",
    "that are relevant to the price and perceived value.\n",
    "\n",
    "Review: ```{prod_review}``` \n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aeb86c-104f-4695-bc4b-b52a05684a07",
   "metadata": {},
   "source": [
    "**2. Information extraction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de89fd7b-5098-4687-877b-90c6c430cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product arrived a day earlier than expected, in good condition.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to extract relevant information from \n",
    "a product review from an ecommerce site to give \n",
    "feedback to the Shipping department.\n",
    "\n",
    "From the review below, delimited by triple quotes \n",
    "extract the information relevant to shipping and delivery. Limit to 30 words.\n",
    "\n",
    "Review: ‚Äú‚Äù‚Äù{prod_review}\"‚Äù‚Äù \n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1d6b4-e461-4dbb-9c13-3c6b4417f12d",
   "metadata": {},
   "source": [
    "**3. Sentiment analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb55d35d-3394-43ce-8c38-4d0e2bb05dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, which is delimited with triple backticks?\n",
    "\n",
    "Give your answer as a single word, either \"positive\", \"negative\" or \"neutral\".\n",
    "\n",
    "Review text: ```{prod_review}```\n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b4127-879b-4f6d-ace0-787d8ed1c185",
   "metadata": {},
   "source": [
    "**4. Entity Extraction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46bd7741-f0d6-49f4-a987-dd14995f5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Company name\": \"Lumina\",\n",
      "  \"Date of contract\": \"unknown\",\n",
      "  \"Sum of contract\": \"unknown\",\n",
      "  \"Currency of a contract\": \"unknown\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "lamp_review = \"\"\"\n",
    "Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\n",
    "Identify the following entities from the text:\n",
    "\n",
    "Company name\n",
    "Date of contract\n",
    "Sum of contract\n",
    "Currency of a contract\n",
    "\n",
    "The review is delimited with triple backticks. \n",
    "Format your response as a JSON object with entities the keys and recoginized entities as values. \n",
    "If the information isn't present, use \"unknown\" as the value. Make your response as short as possible.\n",
    "\n",
    "Text: ```{lamp_review}```\n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b269382-8b80-411b-83ed-3a435a39e34e",
   "metadata": {},
   "source": [
    "**5. Topic recognition (open topics):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06b8d691-ed63-4026-80b1-7a1569b68487",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "In a recent survey conducted by the government, \n",
    "public sector employees were asked to rate their level \n",
    "of satisfaction with the department they work at. \n",
    "The results revealed that NASA was the most popular \n",
    "department with a satisfaction rating of 95%.\n",
    "\n",
    "One NASA employee, John Smith, commented on the findings, \n",
    "stating, \"I'm not surprised that NASA came out on top. \n",
    "It's a great place to work with amazing people and \n",
    "incredible opportunities. I'm proud to be a part of \n",
    "such an innovative organization.\"\n",
    "\n",
    "The results were also welcomed by NASA's management team, \n",
    "with Director Tom Johnson stating, \"We are thrilled to \n",
    "hear that our employees are satisfied with their work at NASA. \n",
    "We have a talented and dedicated team who work tirelessly \n",
    "to achieve our goals, and it's fantastic to see that their \n",
    "hard work is paying off.\"\n",
    "\n",
    "The survey also revealed that the \n",
    "Social Security Administration had the lowest satisfaction \n",
    "rating, with only 45% of employees indicating they were \n",
    "satisfied with their job. The government has pledged to \n",
    "address the concerns raised by employees in the survey and \n",
    "work towards improving job satisfaction across all departments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "feb4ad74-0520-4a6b-b05f-eb722d7fb223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA, satisfaction, John Smith, Tom Johnson, Social Security Administration\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\" \n",
    "Determine five topics that are being discussed in the following text, which is delimited by triple backticks.\n",
    "\n",
    "Make each item one or two words long.\n",
    "\n",
    "Format your response as a list of items separated by commas.\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187751a-b60b-45cd-b445-4eed98435c81",
   "metadata": {},
   "source": [
    "**6. Topic recognition (closed list of topics):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26cc6a67-b9b3-4718-8352-99746a8b6724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA\n"
     ]
    }
   ],
   "source": [
    "topic_list = [\"NASA\", \"Security\", \"Biology\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\n",
    "Recognize a topic from the given topic list delimited in triple backticks \n",
    "that is being discussed in the text, which is delimited by triple quotes. If you cannot determine between these 3, return 'other'.\n",
    "\n",
    "Topic list: ```{topic_list}```\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\n",
    "Provide an answer only with one word, representing a determined topic.\n",
    "\"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052637e-3947-437a-8caf-e517a856f288",
   "metadata": {},
   "source": [
    "**7. Translation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2d70a74-a19b-4a7d-aceb-baf10c6ae1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! ¬øC√≥mo est√°s?\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello! How are you?\"\n",
    "prompt = f\"\"\" Translate the following English text to Spanish: {text}. \"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd71fdc-0388-4180-9cd9-a67c4d3ca96d",
   "metadata": {},
   "source": [
    "**8. Tone transforming:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad4c458f-5922-46eb-9c2f-f286e833937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear [Name],\n",
      "\n",
      "It was a pleasure to see you. I regret to inform you that I will not be able to complete my task by the deadline. Is there any way we can work together to find a solution?\n",
      "\n",
      "Thank you for your understanding.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi, man! Nice to see you. I will not be ready with my task, is it ok?\"\n",
    "prompt = f\"\"\" Translate the following from slang to a business letter: {text}. \"\"\"\n",
    "\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f54a7-b9c6-4872-8c1a-302afb03cfde",
   "metadata": {},
   "source": [
    "**9. Spellcheck / Grammar check:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38712a8d-2a61-4df5-8645-ee58caecb4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, man! Nice to see you. I won't be ready with my task, is that okay?\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi, man! Nise to see you. I wont not be raedy with my task, it is ok?\"\n",
    "\n",
    "prompt = f\"\"\"Proofread and correct the following text. If you don't find any mistakes, just say \"No errors found\". Text: {text}\"\"\"\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f5353-4fe4-48e8-ae6f-d3820ad83455",
   "metadata": {},
   "source": [
    "**10. Multiple tasks at the same time:**  \n",
    "Be aware that a prompt with multiple tasks tends to be less stable and correct than several prompts each for its own task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf0d903f-4359-4c33-98a4-956d6732dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Sentiment\": \"positive\",\n",
      "  \"Anger\": false,\n",
      "  \"Item\": \"lamp\",\n",
      "  \"Brand\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "lamp_review = \"\"\"\n",
    "Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Identify the following items from the review text:\n",
    "\n",
    "Sentiment (positive or negative)\n",
    "Is the reviewer expressing anger? (true or false)\n",
    "Item purchased by reviewer\n",
    "Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \n",
    "Format your response as a JSON object with \"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys. \n",
    "If the information isn't present, use \"unknown\"as the value. \n",
    "Make your response as short as possible. Format the Anger value as a boolean.\n",
    "\n",
    "Review text: '''{lamp_review}''' \n",
    "\"\"\"\n",
    "print(generate_response(prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5ef29-7979-4d8b-9a13-70a7c0d71c6f",
   "metadata": {},
   "source": [
    "These prompts could be a good baseline for your DS/ML task. You can consider them and add your own modifications, just make sure to avoid hallucinations and make them stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ac284-6566-4780-abfb-7f6228b48a92",
   "metadata": {},
   "source": [
    "# Tokenization, advanced:  \n",
    "Here we will understand how text is tokenized before passing to the model with `tiktoken` library from OpenAI.  \n",
    "Source for most of the materials is: [OpenAI official example notebook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c61ae0-3fb7-4baf-ab8b-33886d67898e",
   "metadata": {},
   "source": [
    "## Encodings with tiktoken\n",
    "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) is a fast open-source tokenizer by OpenAI that is used to tokenize texts before passing them into models.  \n",
    "Encodings specify how text is converted into tokens. Different models use different encodings.\n",
    "\n",
    "`tiktoken` supports three encodings used by OpenAI models:\n",
    "\n",
    "| Encoding name           | OpenAI models                                       |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`  |\n",
    "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
    "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
    "\n",
    "You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "```\n",
    "\n",
    "Note that `p50k_base` overlaps substantially with `r50k_base`, and for non-code applications, they will usually give the same tokens.\n",
    "\n",
    "## Tokenizer libraries by language\n",
    "\n",
    "For `cl100k_base` and `p50k_base` encodings:\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
    "\n",
    "For `r50k_base` (`gpt2`) encodings, tokenizers are available in many languages.\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (or alternatively [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
    "\n",
    "\n",
    "## Tokenization with tiktoken:\n",
    "\n",
    "In English, tokens commonly range in length from one character to one word (e.g., `\"t\"` or `\" great\"`), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., `\" is\"` instead of `\"is \"` or `\" \"`+`\"is\"`). You can quickly check how a string is tokenized at the [OpenAI Tokenizer](https://beta.openai.com/tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3eda4-b366-477e-b88d-572cfab0aedd",
   "metadata": {},
   "source": [
    "If needed, install `tiktoken` with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8eb2f651-00b5-46bc-948d-51edc6b317f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.4.0-cp310-cp310-macosx_10_9_x86_64.whl (797 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/anaconda3/lib/python3.10/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/anaconda3/lib/python3.10/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23717e80-9621-4eb9-9c2b-35784bba0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc40e17-9c44-474d-80ec-2c4dc7d3936c",
   "metadata": {},
   "source": [
    "Use `tiktoken.get_encoding()` to load an encoding by name.\n",
    "\n",
    "The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6ae749-bb27-4f29-af34-5dde7c06aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da8915-6d01-4847-b217-9a3cd6c3fcfa",
   "metadata": {},
   "source": [
    "Use `tiktoken.encoding_for_model()` to automatically load the correct encoding for a given model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca13e3d6-7ede-4dae-a719-45304e5ad7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d479c-967d-4c9c-bdda-4acab3b4b886",
   "metadata": {},
   "source": [
    "Turn text into tokens with `encoding.encode()`.\n",
    "\n",
    "The `.encode()` method converts a text string into a list of token integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72beffe-87bd-4b57-a7bb-73a5e646ed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 1609, 5963, 374, 2294, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"tiktoken is great!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9ff80-4ee7-4f0b-8127-7a3e17719aba",
   "metadata": {},
   "source": [
    "Count tokens by counting the length of the list returned by `.encode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3660b0c9-74f4-4ddc-8a8f-2208fc21077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362dbd15-038a-4e31-8cb2-e98900d3e317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6c5b2-d5d4-4a1c-85d4-dbcd5b9e6201",
   "metadata": {},
   "source": [
    "## Turn tokens into text with `encoding.decode()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9afa71-e5f7-48ea-8bc1-cf22c91994d9",
   "metadata": {},
   "source": [
    "`.decode()` converts a list of token integers to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add59e2d-dcba-47e0-bca9-870ec072a166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8856c-ce01-44bd-8dc7-b9b279b7bec0",
   "metadata": {},
   "source": [
    "Warning: although `.decode()` can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78668d1-2bb3-4326-9da3-0ec4c498c57d",
   "metadata": {},
   "source": [
    "For single tokens, `.decode_single_token_bytes()` safely converts a single integer token to the bytes it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc0cf56-adf0-4a24-8ff6-71af5603dd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b't', b'ik', b'token', b' is', b' great', b'!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab6ec7-4158-4aa7-af57-40e6505723f2",
   "metadata": {},
   "source": [
    "(The `b` in front of the strings indicates that the strings are byte strings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e84096-071d-4d18-a088-8b9edb77f83d",
   "metadata": {},
   "source": [
    "## Comparing encodings\n",
    "\n",
    "Different encodings vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464e6106-b0f5-4317-9454-a4b15e5976b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_encodings(example_string: str) -> None:\n",
    "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
    "    # print the example string\n",
    "    print(f'\\nExample string: \"{example_string}\"')\n",
    "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
    "    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        token_integers = encoding.encode(example_string)\n",
    "        num_tokens = len(token_integers)\n",
    "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
    "        print()\n",
    "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "        print(f\"token integers: {token_integers}\")\n",
    "        print(f\"token bytes: {token_bytes}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "755276b8-df8f-4a6d-8bbb-26e9e3b20cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"antidisestablishmentarianism\"\n",
      "\n",
      "gpt2: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "cl100k_base: 6 tokens\n",
      "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
      "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"antidisestablishmentarianism\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2555c628-4fac-4a67-8a55-ec69d99dc83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"2 + 2 = 4\"\n",
      "\n",
      "gpt2: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "cl100k_base: 7 tokens\n",
      "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
      "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"2 + 2 = 4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17ea776b-3137-45f2-9f5c-25389eb9f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"„ÅäË™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„ÅÜ\"\n",
      "\n",
      "gpt2: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "p50k_base: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "cl100k_base: 9 tokens\n",
      "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
      "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"„ÅäË™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„ÅÜ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baea421-ea94-4366-9ea0-de54d872fd0e",
   "metadata": {},
   "source": [
    "In usual scenarios you don't need to bother about tokenization at all: it will be implemented automatically. But tokenization can help you in other aspects of your DS/ML pipeline, for example, when you need to know number of tokens in the input or some parts of the prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a153964-654f-4d2e-84c2-e7ca6d8a5275",
   "metadata": {},
   "source": [
    "# Counting tokens for chat completions API calls\n",
    "\n",
    "ChatGPT models like `gpt-3.5-turbo` and `gpt-4` use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n",
    "\n",
    "Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo` or `gpt-4`.\n",
    "\n",
    "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\n",
    "\n",
    "In particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0bc5ee8-ad03-43b4-8591-4290255e859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de975b8d-de28-4dea-9433-a2929468972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "127 prompt tokens counted by num_tokens_from_messages().\n",
      "127 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-3.5-turbo-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-3.5-turbo\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "127 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0314\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's verify the function above matches the OpenAI API response\n",
    "\n",
    "import openai\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1,  # we're only counting input tokens here, so let's not waste tokens on the output\n",
    "    )\n",
    "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f2e78-74f7-49d7-80c0-6dc99b05ff66",
   "metadata": {},
   "source": [
    "Knowing number of tokens spent is very useful.  \n",
    "**Some examples of tokenization usage in DS/ML pipelines:**\n",
    "- Basic logging of token number in different prompts\n",
    "- Checks for large inputs, then summarization or splitting \n",
    "- Handling long texts like conversation history between User and AI: progressive summarization based on tokens number\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f2f013-35d5-4611-9d92-1318d99c4a90",
   "metadata": {},
   "source": [
    "# Calculate costs for models and prompts:\n",
    "Often, information about your future costs as well as token usage for prompts could be very useful.  \n",
    "For example, you can get insights about what parts of the prompt has the largest number of tokens and reduce some irrelevant information to shorten the prompt.  \n",
    "Having information about the token usage, you can easily estimate your costs. Let's see how to do that with Completion models first.\n",
    "\n",
    "## For Completion models:  \n",
    "For that, you can write a code like in the cells below to calculate costs for Completion models.  \n",
    "Let's assume we need to summary a story, let's estimate our costs. For that we need to use information from [Pricing page on OpenAI](https://openai.com/pricing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "910cb008-08a4-4456-abca-597a957999d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant on 28 Jun, 2023!\n",
    "# Update with the updated costs from https://openai.com/pricing\n",
    "costs_dictionary_completion = {\n",
    "    \"ada\": {\n",
    "        \"usage\": 0.0004,\n",
    "        \"finetuning\": 0.0004,\n",
    "        \"finetuned_usage\": 0.0016,  \n",
    "    },\n",
    "    \"babbage\": {\n",
    "        \"usage\": 0.0005,\n",
    "        \"finetuning\": 0.0006,\n",
    "        \"finetuned_usage\": 0.0024,  \n",
    "    },\n",
    "    \"curie\": {\n",
    "        \"usage\": 0.002,\n",
    "        \"finetuning\": 0.003,\n",
    "        \"finetuned_usage\": 0.012,  \n",
    "    },\n",
    "    \"davinci\": {\n",
    "        \"usage\": 0.02,\n",
    "        \"finetuning\": 0.03,\n",
    "        \"finetuned_usage\": 0.12,  \n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_costs_of_text(text: str):\n",
    "    result_pricings = {}\n",
    "    for model_name in costs_dictionary_completion:\n",
    "        pricings = costs_dictionary_completion[model_name]\n",
    "        encoding = tiktoken.encoding_for_model(model_name)\n",
    "        \n",
    "        tokens_num = len(encoding.encode(text))\n",
    "        result_pricings[model_name] = {\"tokens_num\": tokens_num}\n",
    "        for price_name, price in pricings.items():\n",
    "            result_pricings[model_name][price_name] = price * tokens_num / 1000\n",
    "    return result_pricings  \n",
    "\n",
    "\n",
    "def calculate_costs(inputs, num_tokens_func, costs_dict):\n",
    "    result_pricings = {}\n",
    "    for model_name in costs_dict:\n",
    "        pricings = costs_dict[model_name]\n",
    "\n",
    "        tokens_num = num_tokens_func(inputs, model=model_name)\n",
    "        result_pricings[model_name] = {\"tokens_num\": tokens_num}\n",
    "        for price_name, price in pricings.items():\n",
    "            result_pricings[model_name][price_name] = round(price * tokens_num / 1000, 6)\n",
    "    return result_pricings  \n",
    "\n",
    "\n",
    "def get_n_tokens_completion(text: str, model):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens_num = len(encoding.encode(text))\n",
    "    return tokens_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "248f6306-34bb-4bec-9e25-f08f31449ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Summarize this story:\n",
    "\n",
    "In a recent survey conducted by the government, \n",
    "public sector employees were asked to rate their level \n",
    "of satisfaction with the department they work at. \n",
    "The results revealed that NASA was the most popular \n",
    "department with a satisfaction rating of 95%.\n",
    "\n",
    "One NASA employee, John Smith, commented on the findings, \n",
    "stating, \"I'm not surprised that NASA came out on top. \n",
    "It's a great place to work with amazing people and \n",
    "incredible opportunities. I'm proud to be a part of \n",
    "such an innovative organization.\"\n",
    "\n",
    "The results were also welcomed by NASA's management team, \n",
    "with Director Tom Johnson stating, \"We are thrilled to \n",
    "hear that our employees are satisfied with their work at NASA. \n",
    "We have a talented and dedicated team who work tirelessly \n",
    "to achieve our goals, and it's fantastic to see that their \n",
    "hard work is paying off.\"\n",
    "\n",
    "The survey also revealed that the \n",
    "Social Security Administration had the lowest satisfaction \n",
    "rating, with only 45% of employees indicating they were \n",
    "satisfied with their job. The government has pledged to \n",
    "address the concerns raised by employees in the survey and \n",
    "work towards improving job satisfaction across all departments.\n",
    "\"\"\"\n",
    "\n",
    "completion_costs = calculate_costs(inputs=prompt, num_tokens_func=get_n_tokens_completion, costs_dict=costs_dictionary_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad124ed7-8d41-4c4a-b2c2-51e33435e8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ada\n",
      "- Number of tokens:  271\n",
      "- Cost for usage to the model: 0.000108$\n",
      "- Cost for finetuning to the model: 0.000108$\n",
      "- Cost for finetuned_usage to the model: 0.000434$\n",
      "\n",
      "Model: babbage\n",
      "- Number of tokens:  271\n",
      "- Cost for usage to the model: 0.000136$\n",
      "- Cost for finetuning to the model: 0.000163$\n",
      "- Cost for finetuned_usage to the model: 0.00065$\n",
      "\n",
      "Model: curie\n",
      "- Number of tokens:  271\n",
      "- Cost for usage to the model: 0.000542$\n",
      "- Cost for finetuning to the model: 0.000813$\n",
      "- Cost for finetuned_usage to the model: 0.003252$\n",
      "\n",
      "Model: davinci\n",
      "- Number of tokens:  271\n",
      "- Cost for usage to the model: 0.00542$\n",
      "- Cost for finetuning to the model: 0.00813$\n",
      "- Cost for finetuned_usage to the model: 0.03252$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in completion_costs:\n",
    "    print(f\"Model: {model}\")\n",
    "    for cost_name in completion_costs[model]:\n",
    "        if cost_name  == \"tokens_num\":\n",
    "            print(\"- Number of tokens: \", completion_costs[model][\"tokens_num\"])\n",
    "        else:\n",
    "            print(f\"- Cost for {cost_name} to the model: {completion_costs[model][cost_name]}$\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a6b9f8-6a46-4baa-9043-d65e492793d2",
   "metadata": {},
   "source": [
    "* `Cost for usage to the model` -- cost for passing this prompt to the model\n",
    "* `Cost for finetuning to the model` -- cost for fintuning the model on that text\n",
    "* `Cost for finetuned_usage to the model` -- cost for passing this prompt to the finetuned model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5b6d6-8d1d-4bd9-9fdf-2728a17b38ca",
   "metadata": {},
   "source": [
    "## Calculate costs for ChatCompletion models:  \n",
    "Now let's do the same for ChatCompletion models, but now we have a chat-like messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "88d9bdef-8692-41ac-91e4-6b39921025f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant on 28 Jun, 2023!\n",
    "# Update with the updated costs from https://openai.com/pricing\n",
    "costs_dictionary_chat = {\n",
    "    \"gpt-3.5-turbo\": {\n",
    "        \"input\": 0.0015,\n",
    "    },\n",
    "    \"gpt-3.5-turbo-16k\": {\n",
    "        \"input\": 0.003,\n",
    "    },\n",
    "    \"gpt-4\": {\n",
    "        \"input\": 0.03,\n",
    "    },\n",
    "    \"gpt-4-32k\": {\n",
    "        \"input\": 0.06,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f64d5ab0-a153-4f1d-afb3-562360f342fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9631fafc-778b-4928-ab69-4796aebf3459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n"
     ]
    }
   ],
   "source": [
    "chat_cost = calculate_costs(inputs=example_messages, num_tokens_func=num_tokens_from_messages, costs_dict=costs_dictionary_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3f990b30-a799-47d0-8d00-56ee457c2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-3.5-turbo\n",
      "- Number of tokens:  129\n",
      "- Cost for an input to the model: 0.000194$\n",
      "\n",
      "Model: gpt-3.5-turbo-16k\n",
      "- Number of tokens:  129\n",
      "- Cost for an input to the model: 0.000387$\n",
      "\n",
      "Model: gpt-4\n",
      "- Number of tokens:  129\n",
      "- Cost for an input to the model: 0.00387$\n",
      "\n",
      "Model: gpt-4-32k\n",
      "- Number of tokens:  129\n",
      "- Cost for an input to the model: 0.00774$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in chat_cost:\n",
    "    print(f\"Model: {model}\")\n",
    "    print(\"- Number of tokens: \", chat_cost[model][\"tokens_num\"])\n",
    "    print(f\"- Cost for an input to the model: {chat_cost[model]['input']}$\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5816609-958d-4291-a50c-221b2ba455c8",
   "metadata": {},
   "source": [
    "Having information about money spending and number of tokens will help you to understand and extrapolateyour costs. If you're working with Azure backend, you can see costs analytics directly from Azure project dashboard, but if not, these functions could be very useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
